
import asyncio
import sys
import os

# Add project root to path
sys.path.append(os.getcwd())

from loguru import logger
from agents.search_agent import RealEstateSearchAgent
from storage.database import init_db, close_db, get_session, ListingCRUD, ScrapeLogCRUD
from storage.vector_db import index_listings
from services.validator import get_validator

# Comprehensive list of queries to cover Hanoi market
QUERIES = [
    # Cáº§u Giáº¥y
    "chung cÆ° Cáº§u Giáº¥y 2-4 tá»·",
    "nhÃ  riÃªng Cáº§u Giáº¥y 5-10 tá»·",
    "vÄƒn phÃ²ng cho thuÃª Cáº§u Giáº¥y",
    
    # Äá»‘ng Äa
    "nhÃ  ngÃµ Äá»‘ng Äa 3-6 tá»·",
    "chung cÆ° Äá»‘ng Äa dÆ°á»›i 3 tá»·",
    "nhÃ  máº·t phá»‘ Äá»‘ng Äa kinh doanh",

    # Thanh XuÃ¢n
    "chung cÆ° Thanh XuÃ¢n 3 phÃ²ng ngá»§",
    "nhÃ  riÃªng Thanh XuÃ¢n 4-7 tá»·",
    "chung cÆ° mini Thanh XuÃ¢n cho thuÃª",

    # TÃ¢y Há»“
    "biá»‡t thá»± TÃ¢y Há»“ view há»“",
    "cÄƒn há»™ dá»‹ch vá»¥ TÃ¢y Há»“ cho thuÃª",
    "nhÃ  riÃªng TÃ¢y Há»“ 5-10 tá»·",

    # Long BiÃªn
    "Ä‘áº¥t ná»n Long BiÃªn 30-50m2",
    "liá»n ká» Vinhomes Riverside Long BiÃªn",
    "nhÃ  riÃªng Long BiÃªn dÆ°á»›i 3 tá»·",

    # Nam Tá»« LiÃªm
    "chung cÆ° Má»¹ ÄÃ¬nh 2-3 tá»·",
    "nhÃ  Ä‘áº¥t Nam Tá»« LiÃªm 3-5 tá»·",
    "biá»‡t thá»± Nam Tá»« LiÃªm",

    # HÃ  ÄÃ´ng
    "chung cÆ° HÃ  ÄÃ´ng dÆ°á»›i 2 tá»·",
    "nhÃ  liá»n ká» HÃ  ÄÃ´ng 5-8 tá»·",
    "Ä‘áº¥t dá»‹ch vá»¥ HÃ  ÄÃ´ng",

    # HoÃ ng Mai
    "chung cÆ° HoÃ ng Mai giÃ¡ ráº»",
    "nhÃ  riÃªng HoÃ ng Mai 2-4 tá»·",
    
    # Hai BÃ  TrÆ°ng
    "nhÃ  máº·t phá»‘ Hai BÃ  TrÆ°ng",
    "chung cÆ° cao cáº¥p Hai BÃ  TrÆ°ng",
]

async def main():
    logger.info("ğŸš€ Starting BULK SCRAPE for Data Population...")
    
    # Init DB
    await init_db()
    
    # Init Agent
    agent = RealEstateSearchAgent(headless=True)
    validator = get_validator()

    total_listings = 0
    total_new = 0

    try:
        for idx, query in enumerate(QUERIES):
            logger.info(f"ğŸ” [{idx+1}/{len(QUERIES)}] Scraping: {query}")
            
            try:
                # 1. Search
                result = await agent.search(
                    query,
                    max_results=20, # Get up to 20 per query
                    platforms=["chotot", "batdongsan"],
                )

                if result.listings:
                    # 2. Validate
                    valid_listings, _ = validator.validate_listings(result.listings)
                    
                    # 3. Save to DB
                    new_count = 0
                    async with get_session() as session:
                        for listing in valid_listings:
                            # Generate ID if missing (MD5 of URL)
                            listing_id = listing.get("id")
                            if not listing_id and listing.get("source_url"):
                                import hashlib
                                listing_id = hashlib.md5(listing.get("source_url").encode("utf-8")).hexdigest()

                            _, is_new = await ListingCRUD.upsert(session, {
                                "id": listing_id,
                                "title": listing.get("title"),
                                "description": listing.get("description"),
                                "price_text": listing.get("price_text"),
                                "price_number": listing.get("price_number"),
                                "price_per_m2": listing.get("price_per_m2"),
                                "property_type": listing.get("property_type"),
                                "area_m2": listing.get("area_m2"),
                                "bedrooms": listing.get("bedrooms"),
                                "bathrooms": listing.get("bathrooms"),
                                "address": listing.get("location", {}).get("address"),
                                "ward": listing.get("location", {}).get("ward"),
                                "district": listing.get("location", {}).get("district"),
                                "city": listing.get("location", {}).get("city", "HÃ  Ná»™i"),
                                "contact_name": listing.get("contact", {}).get("name"),
                                "contact_phone": listing.get("contact", {}).get("phone"),
                                "contact_phone_clean": listing.get("contact", {}).get("phone_clean"),
                                "images": listing.get("images", []),
                                "source_url": listing.get("source_url"),
                                "source_platform": listing.get("source_platform"),
                            })
                            if is_new:
                                new_count += 1
                                
                    # 4. Index Vector DB
                    if valid_listings:
                         await index_listings(valid_listings)

                    total_listings += len(valid_listings)
                    total_new += new_count
                    logger.info(f"   âœ… Saved {len(valid_listings)} listings ({new_count} new)")
                else:
                    logger.warning("   âš ï¸ No listings found")

                # Cooldown
                logger.info("â³ Cooling down 5s...")
                await asyncio.sleep(5)

            except Exception as e:
                logger.error(f"âŒ Error scraping '{query}': {e}")
                await asyncio.sleep(5)

    finally:
        await close_db()
        await agent.close()
        logger.info(f"\nğŸ‰ Bulk Scrape Completed!")
        logger.info(f"Total Processed: {total_listings}")
        logger.info(f"Total New Configured: {total_new}")

if __name__ == "__main__":
    # Fix for Windows: Force ProactorEventLoop for Playwright subprocess support
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        asyncio.run(main())
    else:
        asyncio.run(main())
