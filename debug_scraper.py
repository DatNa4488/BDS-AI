"""
Debug Scraper Script.
Scrapes real estate data using the Google-First strategy and saves to the database.
"""

import asyncio
import sys
import asyncio
import hashlib
from datetime import datetime
from loguru import logger

# Force UTF-8 encoding for Windows console
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

from agents.search_agent import RealEstateSearchAgent
from storage.database import get_session, ListingCRUD, ScrapeLogCRUD, ScrapeLog
from storage.vector_db import index_listings
from services.validator import get_validator
from config import settings

# Configure logger
logger.remove()
logger.add(sys.stderr, level="INFO")

async def debug_scrape_and_save():
    """
    Run a debug scrape and save results to the database.
    This ensures manual runs also persist data.
    """
    print("=" * 60)
    print("üõ†Ô∏è  DEBUG SCRAPER & SAVER")
    print("=" * 60)
    
    # 1. Configuration - AUTOMATED for testing
    query = "chung c∆∞ c·∫ßu gi·∫•y"
    max_results = 5
    
    print(f"üìå Query: {query}")
    print(f"üìå Max Results: {max_results}")
    print(f"üìå Headless: {settings.headless_mode}")
    
    # Force disable Google Search to prevent Hallucination (Amazon/Laptop issues)
    # This will use the deterministic direct scraper
    settings.google_search_enabled = False 
    print(f"üìå Google Search: {settings.google_search_enabled} (Forced Disabled for Debug)")
    print("-" * 60)

    # Create ScrapeLog start
    scrape_log_id = None
    async with get_session() as session:
        log = await ScrapeLogCRUD.create(session, {
            "platform": "debug_manual",
            "query": query,
            "status": "running"
        })
        scrape_log_id = log.id

    # 2. Initialize Agent
    print("\nüöÄ Initializing Search Agent...")
    agent = RealEstateSearchAgent(headless=True) # Use headless for speed/stability
    
    try:
        # 3. Perform Search
        print("\nüîç Searching...")
        result = await agent.search(
            query=query,
            max_results=max_results,
            platforms=["chotot", "batdongsan"]
        )
        
        print(f"\n‚úÖ Found {result.total_found} listings from {result.sources_searched}")
        
        if not result.listings:
            print("‚ö†Ô∏è No listings found via Agent. Injecting MOCK data for Analytics testing...")
            # Inject mock data to verify Analytics page
            result.listings = [
                {
                    "title": "Chung c∆∞ C·∫ßu Gi·∫•y 3 t·ª∑ 100m2 full n·ªôi th·∫•t",
                    "price_text": "3 t·ª∑",
                    "price_number": 3000000000,
                    "area_m2": 100,
                    "price_per_m2": 30000000,  # 30tr/m2
                    "location": {"city": "H√† N·ªôi", "district": "C·∫ßu Gi·∫•y", "address": "ƒê∆∞·ªùng C·∫ßu Gi·∫•y"},
                    "source_url": "https://example.com/1",
                    "source_platform": "chotot",
                    "posted_at": datetime.now()
                },
                {
                    "title": "B√°n nh√† C·∫ßu Gi·∫•y 4.5 t·ª∑ ng√µ r·ªông",
                    "price_text": "4.5 t·ª∑",
                    "price_number": 4500000000,
                    "area_m2": 50,
                    "price_per_m2": 90000000,  # 90tr/m2
                    "location": {"city": "H√† N·ªôi", "district": "C·∫ßu Gi·∫•y", "address": "Ng√µ 165 C·∫ßu Gi·∫•y"},
                    "source_url": "https://example.com/2",
                    "source_platform": "batdongsan",
                    "posted_at": datetime.now()
                },
                {
                     "title": "CƒÉn h·ªô cao c·∫•p Indochina Plaza 5 t·ª∑",
                     "price_text": "5 t·ª∑",
                     "price_number": 5000000000,
                     "area_m2": 120,
                     "price_per_m2": 41600000,  # ~41.6tr/m2
                     "location": {"city": "H√† N·ªôi", "district": "C·∫ßu Gi·∫•y", "address": "Xu√¢n Th·ªßy"},
                     "source_url": "https://example.com/3",
                     "source_platform": "batdongsan",
                     "posted_at": datetime.now()
                }
            ]
            result.total_found = len(result.listings)
            # return # Removed return to allow saving

        # 4. Save to Database
        print("\nüíæ Saving to Database...")
        
        saved_count = 0
        new_count = 0
        
        async with get_session() as session:
            for listing in result.listings:
                # Ensure ID exists
                if not listing.get("id"):
                    content = f"{listing.get('source_url', '')}|{listing.get('title', '')}"
                    listing["id"] = hashlib.md5(content.encode()).hexdigest()
                
                # Prepare data for DB (flatten nested structures)
                db_listing = listing.copy()
                
                # Flatten location
                if "location" in db_listing and isinstance(db_listing["location"], dict):
                    loc = db_listing.pop("location")
                    db_listing["address"] = loc.get("address")
                    db_listing["ward"] = loc.get("ward")
                    db_listing["district"] = loc.get("district")
                    db_listing["city"] = loc.get("city", "H√† N·ªôi")

                # Flatten contact
                if "contact" in db_listing and isinstance(db_listing["contact"], dict):
                    contact = db_listing.pop("contact")
                    db_listing["contact_name"] = contact.get("name")
                    db_listing["contact_phone"] = contact.get("phone")
                    db_listing["contact_phone_clean"] = contact.get("contact_phone_clean") or contact.get("phone_clean")

                # Truncate strings ensuring safety
                if db_listing.get("title"): db_listing["title"] = db_listing["title"][:490]
                if db_listing.get("address"): db_listing["address"] = db_listing["address"][:490]
                if db_listing.get("source_url"): db_listing["source_url"] = db_listing["source_url"][:490]
                
                # Upsert to PostgreSQL
                _, is_new = await ListingCRUD.upsert(session, db_listing)
                saved_count += 1
                if is_new:
                    new_count += 1
                    print(f"   [NEW] {listing.get('title')[:50]}...")
                else:
                    print(f"   [UPD] {listing.get('title')[:50]}...")

        print(f"\n‚úÖ Saved {saved_count} listings to PostgreSQL ({new_count} new).")

        # 5. Save to Vector DB
        print("\nüß† Indexing to Vector DB...")
        try:
            await index_listings(result.listings)
            print("‚úÖ Vector indexing complete.")
        except Exception as e:
            print(f"‚ùå Vector indexing failed: {e}")

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        # Log failure
        if scrape_log_id:
            async with get_session() as session:
                await ScrapeLogCRUD.finish(session, scrape_log_id, status="failed", error_message=str(e))
        
    finally:
        await agent.close()
        # Log success if not already failed
        if scrape_log_id and 'new_count' in locals():
            async with get_session() as session:
                await ScrapeLogCRUD.finish(
                    session, 
                    scrape_log_id, 
                    listings_found=result.total_found if 'result' in locals() else 0,
                    listings_new=new_count,
                    status="completed"
                )

        print("\n" + "=" * 60)
        print("‚ú® Done.")

if __name__ == "__main__":
    asyncio.run(debug_scrape_and_save())
